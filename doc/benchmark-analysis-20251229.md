# ローカルLLMベンチマーク分析レポート

**実施日**: 2025年12月29日
**環境**: Apple M4, 16GB RAM, macOS

---

## 総合サマリー

| モデル | 平均速度 | 総トークン | メモリ(MB) | 総時間(秒) |
|--------|---------|-----------|-----------|-----------|
| **llama3.2** | **39.34 t/s** | 1,323 | 6,783 | 33.6 |
| qwen2.5:7b | 19.89 t/s | 1,004 | 4,816 | 50.5 |
| mistral | 19.15 t/s | 1,664 | 7,071 | 86.9 |
| gemma2 | 15.66 t/s | 1,059 | 6,590 | 67.6 |

---

## カテゴリ別速度比較 (tokens/sec)

| カテゴリ | llama3.2 | qwen2.5:7b | mistral | gemma2 |
|---------|----------|------------|---------|--------|
| japanese | **45.2** | 22.2 | 20.3 | 17.1 |
| english | **44.9** | 22.6 | 20.5 | 17.6 |
| translation | **46.2** | 23.0 | 22.3 | 17.5 |
| coding | **44.9** | 21.7 | 21.2 | 16.5 |
| reasoning | **44.5** | 21.2 | 19.4 | 17.4 |
| summarization | **45.6** | 22.4 | 20.9 | 18.2 |
| creative | **48.4** | 26.9 | 21.1 | 17.2 |

---

## 速度ランキング

1. **llama3.2**: 39.34 t/s (圧倒的に高速)
2. qwen2.5:7b: 19.89 t/s
3. mistral: 19.15 t/s
4. gemma2: 15.66 t/s

---

## 品質評価

### 日本語能力

| モデル | 評価 | コメント |
|--------|-----|---------|
| **qwen2.5:7b** | ★★★★★ | 自然な日本語、敬語も的確 |
| gemma2 | ★★★★ | 良好だが絵文字が多め |
| mistral | ★★★ | ローマ字混入あり、やや不自然 |
| llama3.2 | ★★★ | 「リンネット」等の誤り、文法ミス |

### 敬語変換テスト

**入力**: 「明日会議あるから資料作っといて」

| モデル | 出力 | 評価 |
|--------|-----|------|
| qwen2.5:7b | 「明日は会議がございますので、資料の準備をお願い申し上げます。」 | ✅ 適切 |
| gemma2 | 「明日の会議にあたり、資料をご準備いただけますでしょうか。」 | ✅ 適切 |
| llama3.2 | 「次の会議があって、材料を作ってください」 | ❌ 敬語になっていない |
| mistral | 「gozen desu」混入、冗長 | ❌ ローマ字混入 |

### 量子コンピュータ説明（小学生向け）

| モデル | 特徴 |
|--------|-----|
| qwen2.5:7b | 箱の例えを使った分かりやすい説明、「重ね合わせ」を適切に解説 |
| gemma2 | 絵文字付きで親しみやすい、正確な説明 |
| llama3.2 | 説明が長いが「-1」「1/2」など誤った例を含む |
| mistral | 技術用語が多く小学生向けとしては難しい |

### コーディング能力（フィボナッチ関数）

| モデル | 実装方式 | 特徴 |
|--------|---------|-----|
| qwen2.5:7b | イテレーション | 効率的、エラーハンドリング付き |
| gemma2 | 再帰 | 詳細な解説付き |
| llama3.2 | リスト使用 | docstring付き |
| mistral | 再帰 | 例外処理付き |

### 数学問題テスト

**問題**: りんごが5個あります。3個食べたら何個残りますか？計算過程も示してください。

| モデル | 出力 | 評価 |
|--------|-----|------|
| qwen2.5:7b | 「5 - 3 = 2　したがって、食べたら2個のりんごが残ります。」 | ✅ 正解・簡潔 |
| gemma2 | 「5 - 3 を計算すると、2 になります。」 | ✅ 正解・丁寧 |
| llama3.2 | 「リンネットは5個のリンボンがあります...5 - 3 = 2」 | ⚠️ 正解だが「リンネット」「リンボン」と誤記 |
| mistral | 「rippo ga go no hito desu...5 - 3 = 2」 | ⚠️ 正解だがローマ字・英語混入 |

#### 各モデルの回答詳細

**qwen2.5:7b** (89トークン, 4.78秒)
```
はじめに、私たちはりんごの合計数を知っています。それは5個です。
次に、食べたりんごの数を減算します。食べるりんごは3個なので、これを5から引きます。
つまり、計算式は以下のようになります：
5 - 3 = 2
したがって、食べたら2個のりんごが残ります。
```

**gemma2** (87トークン, 5.51秒)
```
計算過程：
* まず、りんごの数を表す数値を代入します。りんごは5個なので、「5」と書き表せます。
* 食べるときは、りんごの数を減らすので、「-3」を加えます。
* 5 - 3 を計算すると、2 になります。

よって、りんごが5個あったら3個食べたら 2 個残ります。
```

**llama3.2** (102トークン, 2.80秒)
```
リンネットは 5 個のリンボンがあります。
1. まず、食べた 3 個を差し引きます。
   5 (リンボンの合計) - 3 (食べたリンボン) = ?
2. を計算してみましょう。
   5 - 3 = 2
したがって、残っているリンゴは 2 個です。
最終的な答えは2です。
```
※「りんご」を「リンネット」「リンボン」と誤認識

**mistral** (191トークン, 10.95秒)
```
rippo ga go no hito desu. san-ma gohan tatta ra nan-ma ni omae iri masu ka?
Ringoo (りんご) が 5 個あります。(There are 5 apples.)
3 個 (san-ma) 食べたら (gohan tatta ra) 残りますか？(How many remain?)
計算過程:
1. Start with 5 apples.
2. Eat 3 apples.
3. Subtract 3 from 5.
   5 - 3 = 2
2 個 (ni-ma) 残ります。(Two apples remain.)
```
※ローマ字と英語が混入、冗長

#### 数学問題の総評

- **全モデルが正解**（2個）を導出
- **品質**: qwen2.5:7b ≈ gemma2 > llama3.2 > mistral
- **速度**: llama3.2 (2.80秒) > qwen2.5:7b (4.78秒) > gemma2 (5.51秒) > mistral (10.95秒)
- llama3.2は高速だが日本語の語彙に問題あり
- mistralは日本語・英語・ローマ字が混在し読みにくい

### 論理推論テスト

**問題**: AはBより背が高い。CはBより背が低い。AとCではどちらが背が高いか？

- 全モデルが正解（A）を導出
- qwen2.5:7bとgemma2は簡潔で分かりやすい説明
- llama3.2とmistralは冗長だが正確

---

## 問題点

### 全モデル共通
- 「英日翻訳」プロンプトで応答なし（0トークン）
- 「コード説明」プロンプトで応答なし（0トークン）

原因として、プロンプトの形式やモデルの設定に起因する可能性あり。

---

## 用途別推奨モデル

| 用途 | 推奨モデル | 理由 |
|-----|----------|------|
| **高速応答が必要** | llama3.2 | 他モデルの2倍以上の速度 |
| **日本語タスク** | qwen2.5:7b | 最も自然で正確な日本語 |
| **メモリ効率重視** | qwen2.5:7b | 4.8GB（最小メモリ使用量） |
| **バランス重視** | gemma2 | 品質と安定性のバランス |
| **長文生成** | mistral | 最多トークン生成 |

---

## 総合結論

### 速度 vs 品質のトレードオフ

```
速度:   llama3.2 >>>> qwen2.5:7b ≈ mistral > gemma2
日本語: qwen2.5:7b > gemma2 > mistral ≈ llama3.2
メモリ: qwen2.5:7b < gemma2 < llama3.2 < mistral
```

### 推奨

1. **日本語を重視する場合**: `qwen2.5:7b`
   - 自然な日本語、敬語対応、最小メモリ

2. **速度を重視する場合**: `llama3.2`
   - 圧倒的な速度、ただし日本語品質は劣る

3. **汎用的に使う場合**: `gemma2`
   - 安定した品質、Google製の信頼性

---

## 次のステップ

1. 応答なしとなったプロンプトの原因調査
2. より大きなモデル（14B等）との比較
3. 長文コンテキストでのテスト
4. 特定ドメイン（コーディング、翻訳）での詳細比較

---

## 付録: 生データ

結果ファイル: `benchmark/results/`

- `gemma2_20251229_223108.json`
- `qwen2.5_7b_20251229_223108.json`
- `llama3.2_20251229_223108.json`
- `mistral_20251229_223108.json`
