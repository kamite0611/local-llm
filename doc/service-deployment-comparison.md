# LLMサービス運用の選択肢比較

ローカルLLMをサービスとして運用する場合の選択肢と、API利用との比較。

## 1. GCPでの運用

### 構成オプション

| 方式 | 月額目安 | 特徴 |
|------|----------|------|
| GCE + T4 | $500-600 | シンプル、固定費 |
| GCE + L4 | $800-1000 | 高速推論 |
| GKE + GPU | $700-1500 | スケーラブル |
| Vertex AI | 従量課金 | マネージド |

### 構成例

**小規模サービス向け:**
- GCE e2-standard-8 + T4 GPU
- Ollama + nginx（リバースプロキシ）
- Cloud Load Balancer

**中規模以上:**
- GKE + GPU ノードプール
- vLLM or TGI（高速推論サーバー）
- Horizontal Pod Autoscaler

### 注意点

- GPU枠の確保: GCPはGPUクォータ申請が必要
- コールドスタート: モデルロードに時間がかかる
- 16GBモデルの場合: 最低T4（16GB VRAM）以上必要

---

## 2. GPU特化クラウド（GCPより安い）

### RunPod / Vast.ai（最安）

```
RunPod:
- A10G (24GB): 約$0.30/時間 ← GCPの半額以下
- RTX 4090: 約$0.40/時間
- サーバーレスGPU対応（使った分だけ課金）

Vast.ai:
- コミュニティGPU（さらに安い）
- RTX 3090: $0.10-0.20/時間
- 信頼性はやや低め
```

### Lambda Labs（安定 + 安い）

```
- A10 (24GB): $0.60/時間
- H100: $2.49/時間
- 専用GPUクラウドで信頼性高め
```

### コスト比較

| サービス | 月額目安 | 特徴 |
|----------|----------|------|
| Vast.ai | $70-150 | 最安、やや不安定 |
| RunPod | $200-300 | 安い、サーバーレス可 |
| Lambda Labs | $400-500 | 安定、GPUクラウド特化 |
| GCP | $500-1000 | 高い、エンタープライズ向け |

---

## 3. マネージドLLM API

自前運用不要の選択肢。

### Together AI / Fireworks AI

```
Together AI:
- Llama 3.1 70B: $0.88/1M tokens
- Qwen, Mistralなど多数対応

Fireworks AI:
- 高速、低レイテンシ
- Llama 3.1 8B: $0.20/1M tokens
```

### Groq（超高速）

```
- 専用LPUで爆速推論
- Llama 3.1 70B: 無料枠あり
- 本番: $0.59/1M tokens
```

---

## 4. Claude / Gemini API との比較

### API料金（1Mトークンあたり）

| モデル | 入力 | 出力 |
|--------|------|------|
| Claude 3.5 Sonnet | $3 | $15 |
| Claude 3.5 Haiku | $0.80 | $4 |
| Gemini 1.5 Flash | $0.075 | $0.30 |
| Gemini 1.5 Pro | $1.25 | $5 |

### 損益分岐点

```
セルフホスト (RunPod A10G): 約$200/月

vs Claude Haiku ($0.80入力, $4出力)
  → 月40M入力トークン以上でセルフホストが有利

vs Gemini Flash ($0.075入力)
  → 月2.6B入力トークン以上で有利（現実的に難しい）
```

### 性能比較

| 項目 | セルフホスト (7-9B) | Claude Sonnet | Gemini Flash |
|------|---------------------|---------------|--------------|
| 品質 | △ | ◎ | ○ |
| 日本語 | △〜○ | ◎ | ○ |
| 速度 | ○ | ○ | ◎ |
| 運用 | 自前 | 不要 | 不要 |

---

## 5. 結論・選択ガイド

### ユースケース別おすすめ

| ユースケース | おすすめ |
|--------------|----------|
| とにかく安く運用 | Vast.ai / RunPod |
| 運用したくない | Together AI / Groq |
| 安定 + 安い | Lambda Labs |
| エンタープライズ要件 | GCP / AWS |

### 月間トークン量別

| 月間トークン | おすすめ |
|--------------|----------|
| 〜100M | Gemini Flash / Claude Haiku |
| 100M〜1B | Together AI / Groq |
| 1B以上 or 規制あり | セルフホスト |

### セルフホストが有利なケース

1. **超大量リクエスト** - 月数億トークン以上
2. **データ規制** - 外部API禁止の要件
3. **カスタマイズ** - ファインチューニング必須
4. **レイテンシ制御** - 専用GPUで安定化

### 現実的な結論

多くのケースでは、7-9Bモデルのセルフホストより **Gemini Flash** や **Claude Haiku** の方が安くて賢い。セルフホストは「学習・検証用」と割り切るのも選択肢。
